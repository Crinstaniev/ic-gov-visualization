{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import string\n",
    "import warnings\n",
    "from multiprocessing.pool import ThreadPool\n",
    "from operator import itemgetter\n",
    "\n",
    "import igviz as ig\n",
    "import networkx as nx\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import plotapi\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "import pyecharts.options as opts\n",
    "import regex\n",
    "import requests\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from plotapi import SplitChord, Terminus\n",
    "from plotly.subplots import make_subplots\n",
    "from pyecharts.charts import WordCloud\n",
    "from tqdm import tqdm\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "nltk.download('stopwords')\n",
    "plotapi.api_key(\"d494c31b-ce51-4470-aa8c-7749ac52ac0b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pio.templates.default = 'ggplot2'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# switch working directory to root\n",
    "while True:\n",
    "    if 'ic-gov-visualization' != os.getcwd().split('/')[-1]:\n",
    "        os.chdir('..')\n",
    "    else:\n",
    "        print(f'working dir: {os.getcwd()}')\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('data'):\n",
    "    os.mkdir('data')\n",
    "\n",
    "if not os.path.exists('figures'):\n",
    "    os.mkdir('figures')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch Proposals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = 'https://ic-api.internetcomputer.org'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get total proposal counts\n",
    "res = requests.get(f'{base_url}/api/nns/proposals-count')\n",
    "proposals_count = json.loads(res.text)['proposals_count']\n",
    "\n",
    "print(f'Total Proposals: {proposals_count}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch proposals\n",
    "def get_proposal(id):\n",
    "    url = base_url + '/api/v3/proposals/' + str(id)\n",
    "    res = requests.get(url)\n",
    "    res_dict = json.loads(res.text)\n",
    "    return res_dict\n",
    "\n",
    "def get_neuron(id):\n",
    "    url = f'https://ic-api.internetcomputer.org/api/v3/neurons/{id}'\n",
    "    res = requests.get(url)\n",
    "    res_dict = json.loads(res.text)\n",
    "    return res_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check proposals fetched last time\n",
    "last_id = None\n",
    "try:\n",
    "    proposal_last_time = json.load(open('data/proposals.json', 'r'))\n",
    "    # convert to pandas dataframe and get the largest id\n",
    "    df = pd.DataFrame(proposal_last_time)\n",
    "    last_id = df['id'].max()\n",
    "    print(f\"last time proposal id fetched: {last_id}\")\n",
    "except Exception as e:\n",
    "    print('proposals file not found')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proposals_count = 2000\n",
    "proposals = []\n",
    "last_id = 0 if last_id is None else last_id\n",
    "proposal_ids = [i for i in range(last_id + 1, proposals_count)]\n",
    "\n",
    "\n",
    "def print_id(id):\n",
    "    try:\n",
    "        res = get_proposal(id)\n",
    "        proposals.append(dict(\n",
    "            id=id,\n",
    "            data=res\n",
    "        ))\n",
    "        print(f'fetched: proposal {id}')\n",
    "    except Exception as e:\n",
    "        print(f'fetch failed: proposal {id}')\n",
    "\n",
    "\n",
    "# enable multithreading for faster download\n",
    "pool = ThreadPool(50)\n",
    "pool.map(print_id, proposal_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if last_id != 0:\n",
    "    last_proposals = pickle.load(open('data/nns_data_raw.pkl', 'rb'))\n",
    "    proposals.extend(last_proposals)\n",
    "    print('proposals appended')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(proposals, open('data/nns_data_raw.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pickle.load(open('data/nns_data_raw.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pickle.load(open('data/nns_data_raw.pkl', 'rb'))\n",
    "data.sort(key=itemgetter('id'))\n",
    "json.dump(data, open('data/proposals.json', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter empty data\n",
    "proposals_no_empty = []\n",
    "\n",
    "for item in data:\n",
    "    if (item['data'].get('code') != 404):\n",
    "        proposals_no_empty.append(\n",
    "            item['data']\n",
    "        )\n",
    "json.dump(proposals_no_empty, open('data/proposals_no_empty.json', 'w'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json('data/proposals_no_empty.json').drop_duplicates(subset=['id'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wordcloud - Proposal Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/proposal_summary_text.txt', 'w') as f:\n",
    "    f.write(' '.join(list(df['summary'].values)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning data for wordcloud\n",
    "df_wordcloud = df[['topic', 'summary']]\n",
    "df_wordcloud = df_wordcloud.groupby('topic').aggregate(' '.join).reset_index()\n",
    "df_wordcloud.to_csv('data/topic_summary_joined.csv')\n",
    "\n",
    "# remove non-utf8 characters\n",
    "\n",
    "\n",
    "def remove_non_utf8(x: str):\n",
    "    x = x.encode('utf-8', errors='ignore').decode('utf-8')\n",
    "    x = regex.sub(r'[^\\x00-\\x7f]', u'', x)\n",
    "    x = regex.sub(r\"http\\S+\", \"\", x)\n",
    "    x = x.translate(str.maketrans('', '', string.punctuation))\n",
    "    x = word_tokenize(x)\n",
    "    x = [word for word in x if not word in stopwords.words('english')]\n",
    "\n",
    "    return (\" \").join(x)\n",
    "\n",
    "\n",
    "df_wordcloud['summary'] = df_wordcloud['summary'].apply(remove_non_utf8)\n",
    "df_wordcloud.to_csv('data/topic_summary_joined.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wordcloud.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_wordcloud(df_wordcloud, topics):\n",
    "\n",
    "    words = []\n",
    "    for topic in topics:\n",
    "        words.append(\n",
    "            df_wordcloud.query(f\"topic == '{topic}'\")['summary'].values[0]\n",
    "        )\n",
    "\n",
    "    all_words = (' ').join(words)\n",
    "\n",
    "    with open('data/all_words.txt', 'w') as f:\n",
    "        f.write(all_words)\n",
    "\n",
    "    wc = WordCloud(\n",
    "        background_color='black',\n",
    "        repeat=True,\n",
    "        width=1024 * 2,\n",
    "        height=768 * 2,\n",
    "        max_words=200,\n",
    "        colormap='Set2',\n",
    "        font_path='fonts/impact.ttf'\n",
    "    )\n",
    "    wc.generate(all_words)\n",
    "\n",
    "    return wc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_word_freq(df_wordcloud, topics):\n",
    "    words = []\n",
    "    for topic in topics:\n",
    "        words.append(\n",
    "            df_wordcloud.query(f\"topic == '{topic}'\")['summary'].values[0]\n",
    "        )\n",
    "\n",
    "    all_words = [ele for ele in words if len(ele) > 20]\n",
    "    all_words = (' ').join(all_words).upper()\n",
    "    all_words = regex.sub(r'\\b[0-9]+\\b\\W*', '', all_words)\n",
    "    all_words = regex.sub(r'(?:^| )\\w(?:$| )', ' ', all_words)\n",
    "\n",
    "    def replace_digit(string):\n",
    "        string = regex.sub(r'\\d', '', string).strip()\n",
    "        return string\n",
    "\n",
    "    all_words = all_words.upper()\n",
    "\n",
    "    tokens = nltk.word_tokenize(all_words)\n",
    "    text = nltk.Text(tokens)\n",
    "    freq = nltk.Counter(text).items()\n",
    "    res = []\n",
    "    for item in freq:\n",
    "        res.append(item)\n",
    "\n",
    "    res.sort(key=lambda x: x[1], reverse=True)\n",
    "    return res\n",
    "\n",
    "\n",
    "data = generate_word_freq(\n",
    "    df_wordcloud=df_wordcloud,\n",
    "    topics=[\n",
    "        # 'TOPIC_EXCHANGE_RATE',\n",
    "        'TOPIC_GOVERNANCE',\n",
    "        # 'TOPIC_KYC',\n",
    "        # 'TOPIC_NETWORK_CANISTER_MANAGEMENT',\n",
    "        # 'TOPIC_NETWORK_ECONOMICS',\n",
    "        # 'TOPIC_NODE_ADMIN',\n",
    "        # 'TOPIC_NODE_PROVIDER_REWARDS',\n",
    "        # 'TOPIC_PARTICIPANT_MANAGEMENT',\n",
    "        'TOPIC_SUBNET_MANAGEMENT',\n",
    "    ]\n",
    ")\n",
    "\n",
    "(\n",
    "    WordCloud(init_opts=opts.InitOpts())\n",
    "    .add(\n",
    "        series_name=\"Keywords\",\n",
    "        data_pair=data,\n",
    "        word_size_range=[20, 80],\n",
    "        textstyle_opts=opts.TextStyleOpts(\n",
    "            font_family=\"impact\",\n",
    "            font_weight='bold',\n",
    "            color='red'\n",
    "        ),\n",
    "        rotate_step=90\n",
    "    )\n",
    "    .set_global_opts(\n",
    "        # title_opts=opts.TitleOpts(\"Theme-dark\"),\n",
    "        title_opts=opts.TitleOpts(\n",
    "            title=\"Keywords\", title_textstyle_opts=opts.TextStyleOpts(font_size=23),\n",
    "        ),\n",
    "        tooltip_opts=opts.TooltipOpts(is_show=True),\n",
    "    )\n",
    "    .render('figures/wordcloud.html')\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Area Chart - Topics River"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_time_df = df[['proposal_id', 'topic',\n",
    "                    'updated_at']].set_index('proposal_id')\n",
    "\n",
    "topic_time_df['updated_at'] = topic_time_df['updated_at'].dt.floor('d')\n",
    "\n",
    "topic_time_stats = topic_time_df.groupby(\n",
    "    'updated_at').value_counts().reset_index()\n",
    "topic_time_stats.columns = ['date', 'topic', 'count']\n",
    "topic_time_stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.area(topic_time_stats, x='date', y='count',\n",
    "              color='topic')\n",
    "fig.update_yaxes(type='log', range=[0, 4])\n",
    "\n",
    "with open('figures/timeriver.html', 'w') as f:\n",
    "    f.write(fig.to_html())\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_time_df = df[['proposal_id', 'topic',\n",
    "                    'updated_at']].set_index('proposal_id')\n",
    "\n",
    "topic_time_df['updated_at'] = topic_time_df['updated_at'].dt.floor('d')\n",
    "\n",
    "topic_time_stats = topic_time_df.groupby(\n",
    "    'updated_at').value_counts().unstack(fill_value=0).reset_index()\n",
    "topics = list(topic_time_stats.columns[1:])\n",
    "\n",
    "topic_time_stats['sum'] = 0\n",
    "\n",
    "for topic in topics:\n",
    "    topic_time_stats['sum'] += topic_time_stats[topic]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = make_subplots(rows=2, cols=1, shared_xaxes=True)\n",
    "\n",
    "for topic in topics:\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=topic_time_stats['updated_at'],\n",
    "        y=topic_time_stats[topic],\n",
    "        mode='lines',\n",
    "        line=dict(width=0.5),\n",
    "        stackgroup='one',\n",
    "        groupnorm='percent',\n",
    "        name=topic,\n",
    "    ), row=2, col=1)\n",
    "\n",
    "fig.add_traces(\n",
    "    go.Line(x=topic_time_stats['updated_at'], y=topic_time_stats['sum'], name='Sum of all Topics'))\n",
    "\n",
    "fig.update_layout(\n",
    "    dict(title='Proposal Topic Changes Over Time', width=1200, height=800))\n",
    "\n",
    "# fig.update_layout(yaxis_range=(0, 100))\n",
    "fig.update_xaxes(title='Time')\n",
    "\n",
    "fig['layout']['yaxis2'].update(title='Percentage', range=[60, 100])\n",
    "fig['layout']['yaxis1'].update(title='Count')\n",
    "\n",
    "fig.show()\n",
    "with open('figures/topic_area_chart.html', 'w') as f:\n",
    "    f.write(fig.to_html())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proposer_network = df[['proposal_id', 'proposer',\n",
    "                       'known_neurons_ballots', 'updated_at']]\n",
    "\n",
    "# drop proposals with no votes\n",
    "proposer_network = proposer_network[proposer_network['known_neurons_ballots'].apply(\n",
    "    len) != 0]\n",
    "proposer_network['proposer'] = proposer_network['proposer'].astype(int)\n",
    "\n",
    "# build proposer roster\n",
    "voters = proposer_network['known_neurons_ballots'].values.flatten()\n",
    "voters_temp = set()\n",
    "voters_id = set()\n",
    "for voter_group in tqdm(voters):\n",
    "    for voter in voter_group:\n",
    "        voters_id.add(int(voter.get('id')))\n",
    "        voter = (voter.get('id'), voter.get('name'))\n",
    "        voters_temp.add(voter)\n",
    "\n",
    "voters = voters_temp\n",
    "\n",
    "proposer_set = set()\n",
    "for proposal in proposer_network.itertuples():\n",
    "    proposer_id = proposal.proposer\n",
    "    proposer_set.add(int(proposer_id))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chord_df = pd.DataFrame(0, columns=list(voters_id), index=list(proposer_set))\n",
    "\n",
    "for proposal in tqdm(proposer_network.itertuples(), total=len(proposer_network)):\n",
    "    voters = proposal.known_neurons_ballots\n",
    "    proposer_id = int(proposal.proposer)\n",
    "    for voter in voters:\n",
    "        voter_id = int(voter.get('id'))\n",
    "        vote = voter.get('vote')\n",
    "        chord_df.loc[proposer_id, voter_id] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chord_df = chord_df[chord_df.sum(axis=1).sort_values() > 100]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = []\n",
    "nodes = []\n",
    "\n",
    "for item in chord_df.itertuples():\n",
    "    proposer = f'Proposer {item.Index}'\n",
    "    nodes.append(dict(\n",
    "        name=proposer,\n",
    "        group='left'\n",
    "    ))\n",
    "    for voter in chord_df.columns:\n",
    "        amount = int(chord_df.loc[item.Index, voter])\n",
    "        voter = f'Voter {voter}'\n",
    "        links.append(dict(\n",
    "                source=voter,\n",
    "                target=proposer,\n",
    "                value=amount\n",
    "        ))\n",
    "\n",
    "for voter in chord_df.columns:\n",
    "    voter = f'Voter {voter}'\n",
    "    nodes.append(dict(\n",
    "        name=voter,\n",
    "        group='right'\n",
    "    ))\n",
    "\n",
    "links = list(filter(lambda x : x.get('value') > 100, links))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(links).sort_values(by='value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SplitChord(\n",
    "    links,\n",
    "    nodes,\n",
    "    # directed=True,\n",
    "    conjunction='to',\n",
    "    verb='give',\n",
    "    noun='votes',\n",
    ").to_html('figures/chord.html')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Terminus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "few_links = list(filter(lambda x: x['value'] > 1000, links))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Terminus(\n",
    "    few_links,\n",
    "    show_stats=False\n",
    ").to_html('figures/terminus.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_network = df[df['topic'] != 'TOPIC_EXCHANGE_RATE']['summary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_all_words = ('. '.join(list(df_network.values)))\n",
    "network_all_words = regex.sub(r'\\b[0-9]+\\b\\W*', '', network_all_words)\n",
    "network_all_words = regex.sub(r'(?:^| )\\w(?:$| )', ' ', network_all_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_url(txt):\n",
    "    url_pattern = regex.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    no_url = url_pattern.sub(r'', txt)\n",
    "\n",
    "    return no_url\n",
    "\n",
    "\n",
    "def stem_tokens(tokens):\n",
    "    stemmer = nltk.stem.PorterStemmer()\n",
    "    tokens = [stemmer.stem(token) for token in tokens]\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def lemmatize_tokens(tokens):\n",
    "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_no_urls = remove_url(network_all_words)\n",
    "text_no_urls = text_no_urls.encode('utf-8', errors='ignore').decode('utf-8')\n",
    "words_in_text = text_no_urls.split()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "words_nsw = [word for word in words_in_text if not word in stop_words]\n",
    "words_nsw = [ele for ele in words_nsw if len(ele) < 20]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "words_nsw_nc = [lemmatizer.lemmatize(w) for w in words_nsw]\n",
    "words_nsw_nc = [\n",
    "    word for word in words_nsw_nc if word not in string.punctuation]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram = list(nltk.bigrams(words_nsw_nc))\n",
    "bigram_counts = collections.Counter(bigram).most_common(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.Graph()\n",
    "for edge in bigram_counts:\n",
    "    link = edge[0]\n",
    "    count = edge[1]\n",
    "    \n",
    "    G.add_edge(link[0], link[1], weight=count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = ig.plot(\n",
    "    G,\n",
    "    node_opacity=0.8,\n",
    ")\n",
    "fig.update_layout(\n",
    "    dict(\n",
    "        width=800,\n",
    "        height=600,\n",
    "    )\n",
    ")\n",
    "\n",
    "with open('figures/network.html', 'w') as f:\n",
    "    f.write(fig.to_html())\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('ic-gov-visualization-TiLus9tN')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "32c1d74bd68d792e3fd7d55c2cfe33a617d1c026abfcb58eaf509fe94673b386"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
